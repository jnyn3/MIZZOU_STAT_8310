---
title: "STAT8310_HW4"
author: "JAYADITYA NATH"
date: "2023-09-21"
output: pdf_document
---


PROBLEM 1.


Part (a)

The seminar leader uses observational data in the given scenario because they have no control on the conditions in which the data is being collected. All the employees of the sample are working under same conditions and belong to one single group.

Part (b)

I can not validate the claim of the seminar leader stating that there is a causal relationship between the increase in employee productivity with the increased class preparation time on the basis of evidence of positive linear statistical relationship between the two variables. This is because linear relationship shows some form of association between two or more variables, which does not always necessarily imply causation.

Part (c)

I think that salary and infrastructure are some variables that might cause both the employee productivity scores and the employee class preparation times to increase (decrease) simultaneously.



Part (d)

Here, we are considering observational data and thus have no control over the behaviour of the data. If the study is changed to an experimental one, the seminar leader will have control over the full procedure where he/she would be able to impose restrictions in several steps, which might form a causal relation between the employee productivity and class preparation time.


PROBLEM 2. 


Loading the dataset in R and renaming the columns containing the x and y variables : 

```{r}
dat_2 = read.table("C:/Users/Jayaditya Nath/Downloads/HW4Pr2Data.txt")
dat_2
View(dat_2)
colnames(dat_2)[1]="y"
colnames(dat_2)[2]="x"
dat_2
```


Part (a)

Assuming the appropriateness of the linear model : 
```{r}
mod2 = lm(y~x,data = dat_2)
summary(mod2)
```

Results : 
$\hat{\beta_{0}} = 2.11405$

$\hat{\beta_{1}} = 0.03883$

Regression line : $\hat{y} = 2.11405 + 0.03883x$


Part (b)

```{r}
plot(dat_2$x,dat_2$y,xlab = "ACT Score",ylab = "GPA")
abline(mod2,col = 'red')
```

Conclusion : From the graph of the estimated regression line, it does not seem that it fits the data well. Also, if we take a look at the R-squared value, it is very less because on introducing the ACT scores as an independent variable, it reduces the variability of GPA(y variable) by only 7% approximately. Again, if we consider the residual standard error, we can conclude that the predicted GPA values are 0.6 units away from the original values on average. Considering the sensitivity of GPA, this value is considerably high. Thus, we can conclude that the estimated regression function does not do a great job in fitting the original data.

Part (c)

```{r}
new = data.frame(x = 30)
predict(mod2,new)
```

For ACT test score of 30, the estimated mean freshman GPA is 3.278863 units.

Part (d)

On the basis of the estimated regression equation, it is reasonable to say that the when the ACT score increases by one point, the mean GPA increases by 0.03883 units.

Part (e)

```{r}
dat_2["residuals"] = mod2$residuals
View(dat_2)
sum(dat_2$residuals)
```

Conclusion : The residuals does not sum to 0 in this case.

Part (f)

```{r}
var(dat_2$residuals)
sd(dat_2$residuals)
```

$\sigma$ is expressed in units of GPA.



PROBLEM 3. 


Loading the data in R : 

```{r}
dat_3 = read.table("C:/Users/Jayaditya Nath/Downloads/HW4Pr3Data.txt")
colnames(dat_3)[1]="y"
colnames(dat_3)[2]="x"
dat_3
View(dat_3)
```



Part (a)

```{r}
mod3 = lm(y~x, data = dat_3)
summary(mod3)
plot(dat_3$x,dat_3$y,xlab = "Percentage having high school diploma",ylab = "Crime rate")
abline(mod3,col = 'red')
```

The estimated regression function is : $\hat{y} = 20517.60 - 170.58x$

From the graph of the estimated regression line, it does not seem that it fits the data well. Also, if we take a look at the R-squared value, it is very less because on introducing the frequency of high-school diploma as an independent variable, it reduces the variability of crime rate(y variable) by only 17% approximately. Again, if we consider the residual standard error, we can conclude that the predicted crime rates are 2356 units away from the original values on average. Considering the maximum value of crime rate, which is 14000, this value is considerably high. Thus, we can conclude that the estimated regression function does not do a great job in fitting the original data.


Part (b)

The estimated difference in the mean crime rates for two countries changes by 170.58 units where high-school graduation rates differ by one percentage.

Part (c)

```{r}
new1 = data.frame(x = 80)
predict(mod3,new1)
```

The estimated mean crime rate last year in countries with 80% high-school graduates was 6871.585 units.


Part (d)

```{r}
mod3$residuals[10]
var(mod3$residuals)
```

Results : 
$\epsilon_{10} = 1401.566$
$\sigma^{2} = 5485219$



PROBLEM 4. 


Part (a)

From the outputs of the given regression analysis, the student concludes that there is a significant linear association between Y and X. I think that this conclusion cannot be warranted because though the estimates are within the computed confidence intervals and thus have a significant value other than 0, this computation is done on an individual basis. Here, we do not know the F-test statistic and its corresponding p-value, which validates the claim of whether a linear association exists between Y and X, when considering the model as a whole.
The implied level of significance is 5%.

Part (b)


Here, we can see that the lower confidence interval for the intercept is negative although, someone can say that dollar sales can not be negative even if the population at a place nears 0. Primarily, the confidence interval including negative values does not imply that the variable associated with the parameter of interest whose confidence interval is being calculated will also be negative. Secondly, we know that the formula for confidence interval is : sample statistic $\pm$ margin of error. Now, it may so happen that the value of the statistic is zero or less than the value of margin of error. In these situations, the lower value of the confidence interval can obviously be negative.



PROBLEM 5.


In the given problem, we are modelling the relation between sales of a a team's project and expenditure. The message displayed is that the more the spending, the fewer is the sales of the product. I think this claim is completely wrong because the two-sided p-value for the estimated slope is 0.91, which is much larger than the 5% level of significance and thus, we fail to reject the null hypothesis. So, we can conclude that the slope does not have any significant value, which in turn claims that there is no linear association between the two variables.
      So, we cannot model any relation between the Y and X variables, thus rejecting the student's claim.



PROBLEM 6.


Using the dataset from PROBLEM 2 : 

Part (a)

```{r}
confint(mod2,'x',level = 0.99)
```

Comment : The confidence interval of the slope parameter does not include 0. The director of admissions might be interested in whether the confidence interval includes 0 because if it does, there is a chance that there might be no relationship between ACT scores and GPA.

Part (b)

```{r}
summary(mod2)
```

The null and alternative hypothesis are : 
$H_{0}$ : no linear relationship exists vs $H_{1}$ : not $H_{0}$

The decision rule : We reject $H_{0}$ if p-value less than or equal to $\alpha$

Conclusion : Looking at the F-statistic and the p-value from the model summary, we observe that the F-statistic value is fairly large and the p-value is relatively smaller. The p-value is much less than the desired level of significance $\alpha = 0.01$ , thus, we can reject the null hypothesis at 1% level of significance and conclude that a linear association exists between student's ACT score and GPA.


Part (c)

```{r}
newc = data.frame(x = 28)
predict(mod2,newc,interval = "confidence",level = 0.95)
```

95% confidence interval of the mean freshman GPA for students whose ACT score is 28 is (3.061384,3.341033)


Part (d)

```{r}
newp = data.frame(x = 28)
predict(mod2,newp,interval = "predict",level = 0.95)
```

95% prediction interval of the mean freshman GPA for students whose ACT score is 28 is (1.959355,4.443063)


Part (e)

The prediction interval in part (d) is wider than the confidence interval in part (c) because in case of prediction interval, as the name suggests, we are making a prediction and thus there is a random error component involved in it. This makes it wider than the confidence interval.


Part (f)


```{r}
new_c_band = data.frame(x=28)
predict(mod2,new_c_band,interval = "confidence",level = 0.95, method = "bonferroni")
```

Comment : The boundary values of the 95% confidence band are 3.061384 and 3.341033, which are similar to those computed for confidence interval in part (c). Here, we have used the Bonferroni's method of adjustment to find the confidence band, which should have been wider. Here, they are similar because this adjustment works well when we are considering multiple comparison, but, in this case it only deals with one single variable.



Part (g)

```{r}
anova(mod2)
```


Part (h)

MSR is the mean squared due to regression and is estimated by the sum squared due to regression standardized by the degrees of freedom of the variables under consideration. It is estimated by the variance of the residuals plus a bias term due to the slope.

MSE is the mean squared due to error and is estimated by the sum squared due to error standardized by the degrees of freedom of the error term. It is estimated by the variance of residuals.

MSR and MSE are equal under the condition $\beta_{1} = 0$


Part (i)


The null and alternative hypothesis are : 
$H_{0} : \beta_{1} = 0$ vs $H_{1} : \beta_{1} \neq 0$

The decision rule : We reject $H_{0}$ if p-value less than or equal to $\alpha$

Conclusion : From the ANOVA table and F-test in the summary of the linear regression model, we can see that the p-value is less than $\alpha = 0.01$ and thus we can reject the null hypothesis at 1% level of significance. We can conclude that slope has a significant value other than 0.


Part (j)

The absolute magnitude of reduction in the variation of Y when X is introduced in the regression model is given by SSR(sum squared due to regression). We obtain its value from the ANOVA table, which is equal to 3.588.

The relative magnitude of reduction in the variation of Y when X is introduced in the regression model is given by SSR(sum squared due to regression)/SSTotal(total sum of squares). We obtain its value from the ANOVA table, which is equal to 0.07262. This is known as coefficient of determination.

Part (k)

```{r}
cor(dat_2$x,dat_2$y)
```

The correlation coefficient is +0.2694818.
